{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7661037b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentencepiece\n",
    "!git clone https://github.com/google-research/bert\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import nltk\n",
    "import random\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "import sentencepiece as spm\n",
    "\n",
    "from glob import glob\n",
    "from google.colab import auth, drive\n",
    "from tensorflow.keras.utils import Progbar\n",
    "\n",
    "sys.path.append(\"bert\")\n",
    "\n",
    "from bert import modeling, optimization, tokenization\n",
    "from bert.run_pretraining import input_fn_builder, model_fn_builder\n",
    "\n",
    "auth.authenticate_user()\n",
    "  \n",
    "# configure logging\n",
    "log = logging.getLogger('tensorflow')\n",
    "log.setLevel(logging.INFO)\n",
    "\n",
    "# create formatter and add it to the handlers\n",
    "formatter = logging.Formatter('%(asctime)s :  %(message)s')\n",
    "sh = logging.StreamHandler()\n",
    "sh.setLevel(logging.INFO)\n",
    "sh.setFormatter(formatter)\n",
    "log.handlers = [sh]\n",
    "\n",
    "if 'COLAB_TPU_ADDR' in os.environ:\n",
    "  log.info(\"Using TPU runtime\")\n",
    "  USE_TPU = True\n",
    "  TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
    "\n",
    "  with tf.Session(TPU_ADDRESS) as session:\n",
    "    log.info('TPU address is ' + TPU_ADDRESS)\n",
    "    # Upload credentials to TPU.\n",
    "    with open('/content/adc.json', 'r') as f:\n",
    "      auth_info = json.load(f)\n",
    "    tf.contrib.cloud.configure_gcs(session, credentials=auth_info)\n",
    "    \n",
    "else:\n",
    "  log.warning('Not connected to TPU runtime')\n",
    "  USE_TPU = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16483a64",
   "metadata": {},
   "source": [
    "## Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390d9bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip\n",
    "!unzip wikitext-103-raw-v1.zip\n",
    "#!tail dataset.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f42df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEMO_MODE = True #@param {type:\"boolean\"}\n",
    "\n",
    "if DEMO_MODE:\n",
    "  CORPUS_SIZE = 1000000\n",
    "else:\n",
    "  CORPUS_SIZE = 100000000 #@param {type: \"integer\"}\n",
    "  \n",
    "!(head -n $CORPUS_SIZE dataset.txt) > subdataset.txt\n",
    "!mv subdataset.txt dataset.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6386747c",
   "metadata": {},
   "source": [
    "## Preprocessing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb29d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_tokenizer = nltk.RegexpTokenizer(\"\\w+\")\n",
    "\n",
    "def normalize_text(text):\n",
    "  # lowercase text\n",
    "  text = str(text).lower()\n",
    "  # remove non-UTF\n",
    "  text = text.encode(\"utf-8\", \"ignore\").decode()\n",
    "  # remove punktuation symbols\n",
    "  text = \" \".join(regex_tokenizer.tokenize(text))\n",
    "  return text\n",
    "\n",
    "def count_lines(filename):\n",
    "    count = 0\n",
    "    with open(filename) as fi:\n",
    "        for line in fi:\n",
    "            count += 1\n",
    "    return count\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1717c654",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_DATA_FPATH = \"dataset.txt\" #@param {type: \"string\"}\n",
    "PRC_DATA_FPATH = \"proc_dataset.txt\" #@param {type: \"string\"}\n",
    "\n",
    "# apply normalization to the dataset\n",
    "# this will take a minute or two\n",
    "\n",
    "total_lines = count_lines(RAW_DATA_FPATH)\n",
    "bar = Progbar(total_lines)\n",
    "\n",
    "with open(RAW_DATA_FPATH,encoding=\"utf-8\") as fi:\n",
    "  with open(PRC_DATA_FPATH, \"w\",encoding=\"utf-8\") as fo:\n",
    "    for l in fi:\n",
    "      fo.write(normalize_text(l)+\"\\n\")\n",
    "      bar.add(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683df42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def joining(word, shin_max, act_len):\n",
    "    return tf.strings.join([*tf.repeat(' ', shin_max - act_len), word])\n",
    "\n",
    "def substr(word, shin_max):\n",
    "    return tf.strings.substr(word, 0, shin_max)\n",
    "\n",
    "def pad_trunc_shingle(t):\n",
    "    shingle_max = 1024\n",
    "    actual_len = tf.strings.length(t)\n",
    "    if_actual_longer = lambda: tf.py_function(joining, inp=[t, shingle_max, actual_len], Tout=[tf.string])\n",
    "    if_word_longer = lambda: tf.py_function(substr, inp=[t, shingle_max], Tout=[tf.string])\n",
    "    return tf.cond(actual_len < shingle_max, if_actual_longer, if_word_longer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e77da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset.txt', 'r') as file:\n",
    "    data = file.read().replace('\\n', '')\n",
    "    \n",
    "data_padded = pad_trunc_shingle(data)\n",
    "\n",
    "with open('dataset.txt', 'w') as f:\n",
    "    f.writelines(data_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513d831d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PREFIX = \"tokenizer\" #@param {type: \"string\"}\n",
    "VOC_SIZE = 32000 #@param {type:\"integer\"}\n",
    "SUBSAMPLE_SIZE = 12800000 #@param {type:\"integer\"}\n",
    "NUM_PLACEHOLDERS = 256 #@param {type:\"integer\"}\n",
    "\n",
    "SPM_COMMAND = ('--input={} --model_prefix={} '\n",
    "               '--vocab_size={} --input_sentence_size={} '\n",
    "               '--shuffle_input_sentence=true ' \n",
    "               '--bos_id=-1 --eos_id=-1').format(\n",
    "               PRC_DATA_FPATH, MODEL_PREFIX, \n",
    "               VOC_SIZE - NUM_PLACEHOLDERS, SUBSAMPLE_SIZE)\n",
    "\n",
    "spm.SentencePieceTrainer.Train(SPM_COMMAND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8847a054",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sentencepiece_vocab(filepath):\n",
    "    voc = []\n",
    "    with open(filepath, encoding='utf-8') as fi:\n",
    "        for line in fi:\n",
    "            voc.append(line.split(\"\\t\")[0])\n",
    "  # skip the first <unk> token\n",
    "        voc = voc[1:]\n",
    "    return voc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84afe3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_sentencepiece_token(token):\n",
    "    if token.startswith(\"‚ñÅ\"):\n",
    "        return token[1:]\n",
    "    else:\n",
    "        return \"##\" + token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8520875b",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_vocab = list(map(parse_sentencepiece_token, snt_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d3a7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctrl_symbols = [\"[PAD]\",\"[UNK]\",\"[CLS]\",\"[SEP]\",\"[MASK]\"]\n",
    "long_vocab = ctrl_symbols + bert_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea578366",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_vocab += [\"[UNUSED_{}]\".format(i) for i in range(VOC_SIZE - len(bert_vocab))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a114d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOC_FNAME = \"vocab.txt\" #@param {type:\"string\"}\n",
    "\n",
    "with open(VOC_FNAME, \"w\") as fo:\n",
    "    for token in bert_vocab:\n",
    "        fo.write(token+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62914250",
   "metadata": {},
   "source": [
    "## Generating Pretraining Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcfd8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ./shards\n",
    "!split -a 4 -l 256000 -d $PRC_DATA_FPATH ./shards/shard_\n",
    "!ls ./shards/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa943f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LENGTH = 512 #@param {type:\"integer\"}\n",
    "MASKED_LM_PROB = 0.15 #@param\n",
    "MAX_PREDICTIONS = 20 #@param {type:\"integer\"}\n",
    "DO_LOWER_CASE = True #@param {type:\"boolean\"}\n",
    "PROCESSES = 2 #@param {type:\"integer\"}\n",
    "PRETRAINING_DIR = \"pretraining_data\" #@param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4654155",
   "metadata": {},
   "outputs": [],
   "source": [
    "XARGS_CMD = (\"ls ./shards/ | \"\n",
    "             \"xargs -n 1 -P {} -I{} \"\n",
    "             \"python3 bert/create_pretraining_data.py \"\n",
    "             \"--input_file=./shards/{} \"\n",
    "             \"--output_file={}/{}.tfrecord \"\n",
    "             \"--vocab_file={} \"\n",
    "             \"--do_lower_case={} \"\n",
    "             \"--max_predictions_per_seq={} \"\n",
    "             \"--max_seq_length={} \"\n",
    "             \"--masked_lm_prob={} \"\n",
    "             \"--random_seed=34 \"\n",
    "             \"--dupe_factor=5\")\n",
    "\n",
    "XARGS_CMD = XARGS_CMD.format(PROCESSES, '{}', '{}', PRETRAINING_DIR, '{}', \n",
    "                             VOC_FNAME, DO_LOWER_CASE, \n",
    "                             MAX_PREDICTIONS, MAX_SEQ_LENGTH, MASKED_LM_PROB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c34145",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.gfile.MkDir(PRETRAINING_DIR)\n",
    "!$XARGS_CMD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1304d6ed",
   "metadata": {},
   "source": [
    "### Pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8640a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"long_resourses\" #@param {type:\"string\"}\n",
    "MODEL_DIR = \"long_model\" #@param {type:\"string\"}\n",
    "tf.gfile.MkDir(MODEL_DIR)\n",
    "\n",
    "if not BUCKET_NAME:\n",
    "  log.warning(\"WARNING: BUCKET_NAME is not set. \"\n",
    "              \"You will not be able to train the model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af17c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LongformerConfig, LongformerModel\n",
    "\n",
    "long_config = LongformerConfig()\n",
    "\n",
    "model = LongformerModel(long_config)\n",
    "\n",
    "long_config  = model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4ca1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data pipeline config\n",
    "TRAIN_BATCH_SIZE = 128 #@param {type:\"integer\"}\n",
    "MAX_PREDICTIONS = 20 #@param {type:\"integer\"}\n",
    "MAX_SEQ_LENGTH = 1024 #@param {type:\"integer\"}\n",
    "MASKED_LM_PROB = 0.15 #@param\n",
    "\n",
    "# Training procedure config\n",
    "EVAL_BATCH_SIZE = 64\n",
    "LEARNING_RATE = 2e-5\n",
    "TRAIN_STEPS = 1000000 #@param {type:\"integer\"}\n",
    "SAVE_CHECKPOINTS_STEPS = 2500 #@param {type:\"integer\"}\n",
    "NUM_TPU_CORES = 8\n",
    "\n",
    "if BUCKET_NAME:\n",
    "  BUCKET_PATH = \"gs://{}\".format(BUCKET_NAME)\n",
    "else:\n",
    "  BUCKET_PATH = \".\"\n",
    "\n",
    "BERT_GCS_DIR = \"{}/{}\".format(BUCKET_PATH, MODEL_DIR)\n",
    "DATA_GCS_DIR = \"{}/{}\".format(BUCKET_PATH, PRETRAINING_DIR)\n",
    "\n",
    "VOCAB_FILE = os.path.join(BERT_GCS_DIR, VOC_FNAME)\n",
    "CONFIG_FILE = os.path.join(BERT_GCS_DIR, \"long_config.json\")\n",
    "\n",
    "INIT_CHECKPOINT = tf.train.latest_checkpoint(BERT_GCS_DIR)\n",
    "\n",
    "bert_config = modeling.BertConfig.from_json_file(CONFIG_FILE)\n",
    "input_files = tf.gfile.Glob(os.path.join(DATA_GCS_DIR,'*tfrecord'))\n",
    "\n",
    "log.info(\"Using checkpoint: {}\".format(INIT_CHECKPOINT))\n",
    "log.info(\"Using {} data shards\".format(len(input_files)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925dc36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fn = model_fn_builder(\n",
    "      bert_config=long_config,\n",
    "      init_checkpoint=INIT_CHECKPOINT,\n",
    "      learning_rate=LEARNING_RATE,\n",
    "      num_train_steps=TRAIN_STEPS,\n",
    "      num_warmup_steps=10,\n",
    "      use_tpu=USE_TPU,\n",
    "      use_one_hot_embeddings=True)\n",
    "\n",
    "tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)\n",
    "\n",
    "run_config = tf.contrib.tpu.RunConfig(\n",
    "    cluster=tpu_cluster_resolver,\n",
    "    model_dir=BERT_GCS_DIR,\n",
    "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,\n",
    "    tpu_config=tf.contrib.tpu.TPUConfig(\n",
    "        iterations_per_loop=SAVE_CHECKPOINTS_STEPS,\n",
    "        num_shards=NUM_TPU_CORES,\n",
    "        per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))\n",
    "\n",
    "estimator = tf.contrib.tpu.TPUEstimator(\n",
    "    use_tpu=USE_TPU,\n",
    "    model_fn=model_fn,\n",
    "    config=run_config,\n",
    "    train_batch_size=TRAIN_BATCH_SIZE,\n",
    "    eval_batch_size=EVAL_BATCH_SIZE)\n",
    "  \n",
    "train_input_fn = input_fn_builder(\n",
    "        input_files=input_files,\n",
    "        max_seq_length=MAX_SEQ_LENGTH,\n",
    "        max_predictions_per_seq=MAX_PREDICTIONS,\n",
    "        is_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2a7ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.train(input_fn=train_input_fn, max_steps=TRAIN_STEPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b684d11",
   "metadata": {},
   "source": [
    "### Mixer Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf2ec31",
   "metadata": {},
   "source": [
    "#### Hypermixer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95682c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"longmixer_resourses\" #@param {type:\"string\"}\n",
    "MODEL_DIR = \"hypermixer\" #@param {type:\"string\"}\n",
    "tf.gfile.MkDir(MODEL_DIR)\n",
    "\n",
    "if not BUCKET_NAME:\n",
    "  log.warning(\"WARNING: BUCKET_NAME is not set. \"\n",
    "              \"You will not be able to train the model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178b5931",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mixer.hypermixer import HyperMixer\n",
    "hype_config = {'dim:'1024}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bc6a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"{}/hyp_config.json\".format(MODEL_DIR), \"w\") as fo:\n",
    "  json.dump(bert_base_config, fo, indent=2)\n",
    "  \n",
    "with open(\"{}/{}\".format(MODEL_DIR, VOC_FNAME), \"w\") as fo:\n",
    "  for token in bert_vocab:\n",
    "    fo.write(token+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71672572",
   "metadata": {},
   "outputs": [],
   "source": [
    "if BUCKET_NAME:\n",
    "  !gsutil -m cp -r $MODEL_DIR $PRETRAINING_DIR gs://$BUCKET_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda49fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"longmixer_resourses\" #@param {type:\"string\"}\n",
    "MODEL_DIR = \"hypermixer\" #@param {type:\"string\"}\n",
    "PRETRAINING_DIR = \"pretraining_data\" #@param {type:\"string\"}\n",
    "VOC_FNAME = \"vocab.txt\" #@param {type:\"string\"}\n",
    "\n",
    "# Input data pipeline config\n",
    "TRAIN_BATCH_SIZE = 128 #@param {type:\"integer\"}\n",
    "MAX_PREDICTIONS = 20 #@param {type:\"integer\"}\n",
    "MAX_SEQ_LENGTH = 512 #@param {type:\"integer\"}\n",
    "MASKED_LM_PROB = 0.15 #@param\n",
    "\n",
    "# Training procedure config\n",
    "EVAL_BATCH_SIZE = 64\n",
    "LEARNING_RATE = 2e-5\n",
    "TRAIN_STEPS = 1000000 #@param {type:\"integer\"}\n",
    "SAVE_CHECKPOINTS_STEPS = 2500 #@param {type:\"integer\"}\n",
    "NUM_TPU_CORES = 8\n",
    "\n",
    "if BUCKET_NAME:\n",
    "  BUCKET_PATH = \"gs://{}\".format(BUCKET_NAME)\n",
    "else:\n",
    "  BUCKET_PATH = \".\"\n",
    "\n",
    "BERT_GCS_DIR = \"{}/{}\".format(BUCKET_PATH, MODEL_DIR)\n",
    "DATA_GCS_DIR = \"{}/{}\".format(BUCKET_PATH, PRETRAINING_DIR)\n",
    "\n",
    "VOCAB_FILE = os.path.join(BERT_GCS_DIR, VOC_FNAME)\n",
    "CONFIG_FILE = os.path.join(BERT_GCS_DIR, \"hyp_config.json\")\n",
    "\n",
    "INIT_CHECKPOINT = tf.train.latest_checkpoint(BERT_GCS_DIR)\n",
    "\n",
    "bert_config = modeling.BertConfig.from_json_file(CONFIG_FILE)\n",
    "input_files = tf.gfile.Glob(os.path.join(DATA_GCS_DIR,'*tfrecord'))\n",
    "\n",
    "log.info(\"Using checkpoint: {}\".format(INIT_CHECKPOINT))\n",
    "log.info(\"Using {} data shards\".format(len(input_files)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e745d8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fn = model_fn_builder(\n",
    "      bert_config=hyp_config,\n",
    "      init_checkpoint=INIT_CHECKPOINT,\n",
    "      learning_rate=LEARNING_RATE,\n",
    "      num_train_steps=TRAIN_STEPS,\n",
    "      num_warmup_steps=10,\n",
    "      use_tpu=USE_TPU,\n",
    "      use_one_hot_embeddings=True)\n",
    "\n",
    "tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)\n",
    "\n",
    "run_config = tf.contrib.tpu.RunConfig(\n",
    "    cluster=tpu_cluster_resolver,\n",
    "    model_dir=BERT_GCS_DIR,\n",
    "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,\n",
    "    tpu_config=tf.contrib.tpu.TPUConfig(\n",
    "        iterations_per_loop=SAVE_CHECKPOINTS_STEPS,\n",
    "        num_shards=NUM_TPU_CORES,\n",
    "        per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))\n",
    "\n",
    "estimator = tf.contrib.tpu.TPUEstimator(\n",
    "    use_tpu=USE_TPU,\n",
    "    model_fn=model_fn,\n",
    "    config=run_config,\n",
    "    train_batch_size=TRAIN_BATCH_SIZE,\n",
    "    eval_batch_size=EVAL_BATCH_SIZE)\n",
    "  \n",
    "train_input_fn = input_fn_builder(\n",
    "        input_files=input_files,\n",
    "        max_seq_length=MAX_SEQ_LENGTH,\n",
    "        max_predictions_per_seq=MAX_PREDICTIONS,\n",
    "        is_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f138e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.train(input_fn=train_input_fn, max_steps=TRAIN_STEPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551adf83",
   "metadata": {},
   "source": [
    "#### Paramixer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae29e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"longmixer_resourses\" #@param {type:\"string\"}\n",
    "MODEL_DIR = \"paramixer\" #@param {type:\"string\"}\n",
    "tf.gfile.MkDir(MODEL_DIR)\n",
    "\n",
    "if not BUCKET_NAME:\n",
    "  log.warning(\"WARNING: BUCKET_NAME is not set. \"\n",
    "              \"You will not be able to train the model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7a3b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mixer.paramixer import Paramixer\n",
    "\n",
    "para_config = {\n",
    "    'vocab_size':VOC_SIZE,\n",
    "    'embedding_size':1024,\n",
    "    'max_seq_len':1024,\n",
    "    'n_layers':12,          \n",
    "    'dropout1_p':0.3,\n",
    "    'dropout2_p':0.4,\n",
    "    'pooling_type':'maxpool',\n",
    "    'hidden_size':768\n",
    "}\n",
    "\n",
    "\n",
    "with open(\"{}/para_config.json\".format(MODEL_DIR), \"w\") as fo:\n",
    "  json.dump(bert_base_config, fo, indent=2)\n",
    "  \n",
    "with open(\"{}/{}\".format(MODEL_DIR, VOC_FNAME), \"w\") as fo:\n",
    "  for token in bert_vocab:\n",
    "    fo.write(token+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daed2e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "if BUCKET_NAME:\n",
    "  !gsutil -m cp -r $MODEL_DIR $PRETRAINING_DIR gs://$BUCKET_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0fc734",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"longmixer_resourses\" #@param {type:\"string\"}\n",
    "MODEL_DIR = \"paramixer\" #@param {type:\"string\"}\n",
    "PRETRAINING_DIR = \"pretraining_data\" #@param {type:\"string\"}\n",
    "VOC_FNAME = \"vocab.txt\" #@param {type:\"string\"}\n",
    "\n",
    "# Input data pipeline config\n",
    "TRAIN_BATCH_SIZE = 128 #@param {type:\"integer\"}\n",
    "MAX_PREDICTIONS = 20 #@param {type:\"integer\"}\n",
    "MAX_SEQ_LENGTH = 1024 #@param {type:\"integer\"}\n",
    "MASKED_LM_PROB = 0.15 #@param\n",
    "\n",
    "# Training procedure config\n",
    "EVAL_BATCH_SIZE = 64\n",
    "LEARNING_RATE = 2e-5\n",
    "TRAIN_STEPS = 1000000 #@param {type:\"integer\"}\n",
    "SAVE_CHECKPOINTS_STEPS = 2500 #@param {type:\"integer\"}\n",
    "NUM_TPU_CORES = 8\n",
    "\n",
    "if BUCKET_NAME:\n",
    "  BUCKET_PATH = \"gs://{}\".format(BUCKET_NAME)\n",
    "else:\n",
    "  BUCKET_PATH = \".\"\n",
    "\n",
    "BERT_GCS_DIR = \"{}/{}\".format(BUCKET_PATH, MODEL_DIR)\n",
    "DATA_GCS_DIR = \"{}/{}\".format(BUCKET_PATH, PRETRAINING_DIR)\n",
    "\n",
    "VOCAB_FILE = os.path.join(BERT_GCS_DIR, VOC_FNAME)\n",
    "CONFIG_FILE = os.path.join(BERT_GCS_DIR, \"para_config.json\")\n",
    "\n",
    "INIT_CHECKPOINT = tf.train.latest_checkpoint(BERT_GCS_DIR)\n",
    "\n",
    "bert_config = modeling.BertConfig.from_json_file(CONFIG_FILE)\n",
    "input_files = tf.gfile.Glob(os.path.join(DATA_GCS_DIR,'*tfrecord'))\n",
    "\n",
    "log.info(\"Using checkpoint: {}\".format(INIT_CHECKPOINT))\n",
    "log.info(\"Using {} data shards\".format(len(input_files)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7baf8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fn = model_fn_builder(\n",
    "      bert_config=para_config,\n",
    "      init_checkpoint=INIT_CHECKPOINT,\n",
    "      learning_rate=LEARNING_RATE,\n",
    "      num_train_steps=TRAIN_STEPS,\n",
    "      num_warmup_steps=10,\n",
    "      use_tpu=USE_TPU,\n",
    "      use_one_hot_embeddings=True)\n",
    "\n",
    "tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)\n",
    "\n",
    "run_config = tf.contrib.tpu.RunConfig(\n",
    "    cluster=tpu_cluster_resolver,\n",
    "    model_dir=BERT_GCS_DIR,\n",
    "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,\n",
    "    tpu_config=tf.contrib.tpu.TPUConfig(\n",
    "        iterations_per_loop=SAVE_CHECKPOINTS_STEPS,\n",
    "        num_shards=NUM_TPU_CORES,\n",
    "        per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))\n",
    "\n",
    "estimator = tf.contrib.tpu.TPUEstimator(\n",
    "    use_tpu=USE_TPU,\n",
    "    model_fn=model_fn,\n",
    "    config=run_config,\n",
    "    train_batch_size=TRAIN_BATCH_SIZE,\n",
    "    eval_batch_size=EVAL_BATCH_SIZE)\n",
    "  \n",
    "train_input_fn = input_fn_builder(\n",
    "        input_files=input_files,\n",
    "        max_seq_length=MAX_SEQ_LENGTH,\n",
    "        max_predictions_per_seq=MAX_PREDICTIONS,\n",
    "        is_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66d621c",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.train(input_fn=train_input_fn, max_steps=TRAIN_STEPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369bb48f",
   "metadata": {},
   "source": [
    "#### MorphMLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5810a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"longmixer_resourses\" #@param {type:\"string\"}\n",
    "MODEL_DIR = \"morphmixer\" #@param {type:\"string\"}\n",
    "tf.gfile.MkDir(MODEL_DIR)\n",
    "\n",
    "if not BUCKET_NAME:\n",
    "  log.warning(\"WARNING: BUCKET_NAME is not set. \"\n",
    "              \"You will not be able to train the model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19981c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mixer.MorphMLP import MorphMLP\n",
    "\n",
    "morph_config  = {'dim:'512}\n",
    "\n",
    "\n",
    "with open(\"{}/morph_config.json\".format(MODEL_DIR), \"w\") as fo:\n",
    "  json.dump(bert_base_config, fo, indent=2)\n",
    "  \n",
    "with open(\"{}/{}\".format(MODEL_DIR, VOC_FNAME), \"w\") as fo:\n",
    "  for token in bert_vocab:\n",
    "    fo.write(token+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb21429",
   "metadata": {},
   "outputs": [],
   "source": [
    "if BUCKET_NAME:\n",
    "  !gsutil -m cp -r $MODEL_DIR $PRETRAINING_DIR gs://$BUCKET_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514dcf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"mixer_resourses\" #@param {type:\"string\"}\n",
    "MODEL_DIR = \"morphmixer\" #@param {type:\"string\"}\n",
    "PRETRAINING_DIR = \"pretraining_data\" #@param {type:\"string\"}\n",
    "VOC_FNAME = \"vocab.txt\" #@param {type:\"string\"}\n",
    "\n",
    "# Input data pipeline config\n",
    "TRAIN_BATCH_SIZE = 64 #@param {type:\"integer\"}\n",
    "MAX_PREDICTIONS = 20 #@param {type:\"integer\"}\n",
    "MAX_SEQ_LENGTH = 1024 #@param {type:\"integer\"}\n",
    "MASKED_LM_PROB = 0.15 #@param\n",
    "\n",
    "# Training procedure config\n",
    "EVAL_BATCH_SIZE = 64\n",
    "LEARNING_RATE = 2e-5\n",
    "TRAIN_STEPS = 1000000 #@param {type:\"integer\"}\n",
    "SAVE_CHECKPOINTS_STEPS = 2500 #@param {type:\"integer\"}\n",
    "NUM_TPU_CORES = 8\n",
    "\n",
    "if BUCKET_NAME:\n",
    "  BUCKET_PATH = \"gs://{}\".format(BUCKET_NAME)\n",
    "else:\n",
    "  BUCKET_PATH = \".\"\n",
    "\n",
    "BERT_GCS_DIR = \"{}/{}\".format(BUCKET_PATH, MODEL_DIR)\n",
    "DATA_GCS_DIR = \"{}/{}\".format(BUCKET_PATH, PRETRAINING_DIR)\n",
    "\n",
    "VOCAB_FILE = os.path.join(BERT_GCS_DIR, VOC_FNAME)\n",
    "CONFIG_FILE = os.path.join(BERT_GCS_DIR, \"para_config.json\")\n",
    "\n",
    "INIT_CHECKPOINT = tf.train.latest_checkpoint(BERT_GCS_DIR)\n",
    "\n",
    "bert_config = modeling.BertConfig.from_json_file(CONFIG_FILE)\n",
    "input_files = tf.gfile.Glob(os.path.join(DATA_GCS_DIR,'*tfrecord'))\n",
    "\n",
    "log.info(\"Using checkpoint: {}\".format(INIT_CHECKPOINT))\n",
    "log.info(\"Using {} data shards\".format(len(input_files)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542194db",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fn = model_fn_builder(\n",
    "      bert_config=morph_config,\n",
    "      init_checkpoint=INIT_CHECKPOINT,\n",
    "      learning_rate=LEARNING_RATE,\n",
    "      num_train_steps=TRAIN_STEPS,\n",
    "      num_warmup_steps=10,\n",
    "      use_tpu=USE_TPU,\n",
    "      use_one_hot_embeddings=True)\n",
    "\n",
    "tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)\n",
    "\n",
    "run_config = tf.contrib.tpu.RunConfig(\n",
    "    cluster=tpu_cluster_resolver,\n",
    "    model_dir=BERT_GCS_DIR,\n",
    "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,\n",
    "    tpu_config=tf.contrib.tpu.TPUConfig(\n",
    "        iterations_per_loop=SAVE_CHECKPOINTS_STEPS,\n",
    "        num_shards=NUM_TPU_CORES,\n",
    "        per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))\n",
    "\n",
    "estimator = tf.contrib.tpu.TPUEstimator(\n",
    "    use_tpu=USE_TPU,\n",
    "    model_fn=model_fn,\n",
    "    config=run_config,\n",
    "    train_batch_size=TRAIN_BATCH_SIZE,\n",
    "    eval_batch_size=EVAL_BATCH_SIZE)\n",
    "  \n",
    "train_input_fn = input_fn_builder(\n",
    "        input_files=input_files,\n",
    "        max_seq_length=MAX_SEQ_LENGTH,\n",
    "        max_predictions_per_seq=MAX_PREDICTIONS,\n",
    "        is_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2de259d",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.train(input_fn=train_input_fn, max_steps=TRAIN_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8db5691",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6208425d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
