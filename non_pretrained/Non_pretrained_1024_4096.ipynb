{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccac4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from official.nlp import optimization  # to create AdamW optimizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ee7558",
   "metadata": {},
   "outputs": [],
   "source": [
    "InvalidArgumentError: Unimplemented: File system scheme '[local]' not implemented\n",
    "\n",
    "os.environ[\"TFHUB_MODEL_LOAD_FORMAT\"]=\"UNCOMPRESSED\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e350707e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if os.environ['COLAB_TPU_ADDR']:\n",
    "  cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\n",
    "  tf.config.experimental_connect_to_cluster(cluster_resolver)\n",
    "  tf.tpu.experimental.initialize_tpu_system(cluster_resolver)\n",
    "  strategy = tf.distribute.TPUStrategy(cluster_resolver)\n",
    "  print('Using TPU')\n",
    "elif tf.config.list_physical_devices('GPU'):\n",
    "  strategy = tf.distribute.MirroredStrategy()\n",
    "  print('Using GPU')\n",
    "else:\n",
    "  raise ValueError('Running on CPU is not recommended.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fbd754",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_model_name = 'longformer_wikisplit' \n",
    "'\n",
    "map_name_to_handle = {\n",
    "    'bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\n",
    "    'bert_en_uncased_L-24_H-1024_A-16':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/3',\n",
    "    'bert_en_wwm_uncased_L-24_H-1024_A-16':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_wwm_uncased_L-24_H-1024_A-16/3',\n",
    "    'bert_en_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\n",
    "    'bert_en_cased_L-24_H-1024_A-16':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_cased_L-24_H-1024_A-16/3',\n",
    "    'bert_en_wwm_cased_L-24_H-1024_A-16':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_wwm_cased_L-24_H-1024_A-16/3',\n",
    "    'bert_multi_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',\n",
    "    'albert_en_base':\n",
    "        'https://tfhub.dev/tensorflow/albert_en_base/2',\n",
    "    'albert_en_large':\n",
    "        'https://tfhub.dev/tensorflow/albert_en_large/2',\n",
    "    'albert_en_xlarge':\n",
    "        'https://tfhub.dev/tensorflow/albert_en_xlarge/2',\n",
    "    'albert_en_xxlarge':\n",
    "        'https://tfhub.dev/tensorflow/albert_en_xxlarge/2',\n",
    "    'electra_small':\n",
    "        'https://tfhub.dev/google/electra_small/2',\n",
    "    'electra_base':\n",
    "        'https://tfhub.dev/google/electra_base/2',\n",
    "    'experts_pubmed':\n",
    "        'https://tfhub.dev/google/experts/bert/pubmed/2',\n",
    "    'experts_wiki_books':\n",
    "        'https://tfhub.dev/google/experts/bert/wiki_books/2',\n",
    "    'talking-heads_base':\n",
    "        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\n",
    "    'talking-heads_large':\n",
    "        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_large/1',\n",
    "    'roberta_gigaword':\n",
    "         'https://tfhub.dev/google/bertseq2seq/roberta24_gigaword/1',\n",
    "    'longformer_wikisplit':\n",
    "          'https://tfhub.dev/google/bertseq2seq/longformer_wikisplit/1'\n",
    "          \n",
    "    \n",
    "}\n",
    "\n",
    "map_model_to_preprocess = {\n",
    "    'bert_en_uncased_L-24_H-1024_A-16':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'bert_en_wwm_cased_L-24_H-1024_A-16':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n",
    "    'bert_en_cased_L-24_H-1024_A-16':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n",
    "    'bert_en_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n",
    "    'bert_en_wwm_uncased_L-24_H-1024_A-16':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'bert_multi_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3',\n",
    "    'albert_en_base':\n",
    "        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n",
    "    'albert_en_large':\n",
    "        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n",
    "    'albert_en_xlarge':\n",
    "        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n",
    "    'albert_en_xxlarge':\n",
    "        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n",
    "    'electra_small':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'electra_base':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'experts_pubmed':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'experts_wiki_books':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'talking-heads_base':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'talking-heads_large':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'roberta_gigaword':\n",
    "         'https://tfhub.dev/google/bertseq2seq/roberta24_gigaword/1',\n",
    "    'longformer_wikisplit':\n",
    "          'https://tfhub.dev/google/bertseq2seq/longformer_wikisplit/1'\n",
    "}\n",
    "\n",
    "tfhub_handle_encoder = map_name_to_handle[long_model_name]\n",
    "tfhub_handle_preprocess = map_model_to_preprocess[long_model_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a47629a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_long_preprocess_model(sentence_features, seq_length=1024):\n",
    " \n",
    "  input_segments = [\n",
    "      tf.keras.layers.Input(shape=(), dtype=tf.string, name=ft)\n",
    "      for ft in sentence_features]\n",
    "\n",
    "  # Tokenize the text to word pieces.\n",
    "  bert_preprocess = hub.load(tfhub_handle_preprocess)\n",
    "  tokenizer = hub.KerasLayer(bert_preprocess.tokenize, name='tokenizer')\n",
    "  segments = [tokenizer(s) for s in input_segments]\n",
    "\n",
    "  # Optional: Trim segments in a smart way to fit seq_length.\n",
    "  # Simple cases (like this example) can skip this step and let\n",
    "  # the next step apply a default truncation to approximately equal lengths.\n",
    "  truncated_segments = segments\n",
    "\n",
    "  # Pack inputs. The details (start/end token ids, dict of output tensors)\n",
    "  # are model-dependent, so this gets loaded from the SavedModel.\n",
    "  packer = hub.KerasLayer(bert_preprocess.bert_pack_inputs,\n",
    "                          arguments=dict(seq_length=seq_length),\n",
    "                          name='packer')\n",
    "  model_inputs = packer(truncated_segments)\n",
    "  return tf.keras.Model(input_segments, model_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f4e418",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "\n",
    "def load_dataset_from_tfds(in_memory_ds, info, split, batch_size,\n",
    "                           bert_preprocess_model):\n",
    "  is_training = split.startswith('train')\n",
    "  dataset = tf.data.Dataset.from_tensor_slices(in_memory_ds[split])\n",
    "  num_examples = info.splits[split].num_examples\n",
    "\n",
    "  if is_training:\n",
    "    dataset = dataset.shuffle(num_examples)\n",
    "    dataset = dataset.repeat()\n",
    "  dataset = dataset.batch(batch_size)\n",
    "  dataset = dataset.map(lambda ex: (bert_preprocess_model(ex), ex['label']))\n",
    "  dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "  return dataset, num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2274bcd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0345f72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier_model(num_classes):\n",
    "\n",
    "  class Classifier(tf.keras.Model):\n",
    "    def __init__(self, num_classes):\n",
    "      super(Classifier, self).__init__(name=\"prediction\")\n",
    "      self.encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True)\n",
    "      self.dropout = tf.keras.layers.Dropout(0.1)\n",
    "      self.dense = tf.keras.layers.Dense(num_classes)\n",
    "\n",
    "    def call(self, preprocessed_text):\n",
    "      encoder_outputs = self.encoder(preprocessed_text)\n",
    "      pooled_output = encoder_outputs[\"pooled_output\"]\n",
    "      x = self.dropout(pooled_output)\n",
    "      x = self.dense(x)\n",
    "      return x\n",
    "\n",
    "  model = Classifier(num_classes)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3331c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfds_name = 'scrolls/contract_nli' #change according to the task\n",
    "\n",
    "tfds_info = tfds.builder(tfds_name).info\n",
    "\n",
    "sentence_features = list(tfds_info.features.keys())\n",
    "sentence_features.remove('idx')\n",
    "sentence_features.remove('label')\n",
    "\n",
    "available_splits = list(tfds_info.splits.keys())\n",
    "train_split = 'train'\n",
    "validation_split = 'validation'\n",
    "test_split = 'test'\n",
    "if tfds_name == 'scrolls/contract_nli':\n",
    "  validation_split = 'validation_matched'\n",
    "  test_split = 'test_matched'\n",
    "\n",
    "num_classes = tfds_info.features['label'].num_classes\n",
    "num_examples = tfds_info.splits.total_num_examples\n",
    "\n",
    "print(f'Using {tfds_name} from TFDS')\n",
    "print(f'This dataset has {num_examples} examples')\n",
    "print(f'Number of classes: {num_classes}')\n",
    "print(f'Features {sentence_features}')\n",
    "print(f'Splits {available_splits}')\n",
    "\n",
    "with tf.device('/job:localhost'):\n",
    "  # batch_size=-1 is a way to load the dataset into memory\n",
    "  in_memory_ds = tfds.load(tfds_name, batch_size=-1, shuffle_files=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fa785a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact_match_fn(y_true, y_logits, threshold):\n",
    "    #pred = tf.equal(tf.round(y_logits), tf.round(y_true))\n",
    "    predictions = tf.to_float(tf.greater_equal(y_logits, threshold))\n",
    "    pred_match = tf.equal(predictions, tf.round(y_true))\n",
    "    exact_match = tf.reduce_min(tf.to_float(pred_match), axis=1)\n",
    "    return exact_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf48c389",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_configuration(glue_task):\n",
    "\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "    metrics = exact_match()\n",
    "    \n",
    "    return loss\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9ab1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "batch_size = 32\n",
    "init_lr = 2e-5\n",
    "\n",
    "print(f'Fine tuning {tfhub_handle_encoder} model')\n",
    "bert_preprocess_model = make_bert_preprocess_model(sentence_features)\n",
    "\n",
    "with strategy.scope():\n",
    "\n",
    "  # metric have to be created inside the strategy scope\n",
    "  metrics, loss = get_configuration(tfds_name)\n",
    "\n",
    "  train_dataset, train_data_size = load_dataset_from_tfds(\n",
    "      in_memory_ds, tfds_info, train_split, batch_size, bert_preprocess_model)\n",
    "  steps_per_epoch = train_data_size // batch_size\n",
    "  num_train_steps = steps_per_epoch * epochs\n",
    "  num_warmup_steps = num_train_steps // 10\n",
    "\n",
    "  validation_dataset, validation_data_size = load_dataset_from_tfds(\n",
    "      in_memory_ds, tfds_info, validation_split, batch_size,\n",
    "      bert_preprocess_model)\n",
    "  validation_steps = validation_data_size // batch_size\n",
    "\n",
    "  classifier_model = build_classifier_model(num_classes)\n",
    "\n",
    "  optimizer = optimization.create_optimizer(\n",
    "      init_lr=init_lr,\n",
    "      num_train_steps=num_train_steps,\n",
    "      num_warmup_steps=num_warmup_steps,\n",
    "      optimizer_type='adamw')\n",
    "\n",
    "  classifier_model.compile(optimizer=optimizer, loss=loss, metrics=exact_match_fn)\n",
    "\n",
    "  classifier_model.fit(\n",
    "      x=train_dataset,\n",
    "      validation_data=validation_dataset,\n",
    "      steps_per_epoch=steps_per_epoch,\n",
    "      epochs=epochs,\n",
    "      validation_steps=validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5cd0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_save_path = './my_models'\n",
    "bert_type = tfhub_handle_encoder.split('/')[-2]\n",
    "saved_model_name = f'{tfds_name.replace(\"/\", \"_\")}_{bert_type}'\n",
    "\n",
    "saved_model_path = os.path.join(main_save_path, saved_model_name)\n",
    "\n",
    "preprocess_inputs = bert_preprocess_model.inputs\n",
    "bert_encoder_inputs = bert_preprocess_model(preprocess_inputs)\n",
    "bert_outputs = classifier_model(bert_encoder_inputs)\n",
    "model_for_export = tf.keras.Model(preprocess_inputs, bert_outputs)\n",
    "\n",
    "print('Saving', saved_model_path)\n",
    "\n",
    "# Save everything on the Colab host (even the variables from TPU memory)\n",
    "save_options = tf.saved_model.SaveOptions(experimental_io_device='/job:localhost')\n",
    "model_for_export.save(saved_model_path, include_optimizer=False,\n",
    "                      options=save_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac8d8e1",
   "metadata": {},
   "source": [
    "### Hypermixer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae94a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mixer.hypermixer import HyperMixer\n",
    "hyper_mixer_block = HyperMixerBlock(dim=512)\n",
    "\n",
    "def build_classifier_model(num_classes):\n",
    "\n",
    "  class Classifier(tf.keras.Model):\n",
    "    def __init__(self, num_classes):\n",
    "      super(Classifier, self).__init__(name=\"prediction\")\n",
    "      self.encoder = hyper_mixer_block\n",
    "      self.dropout = tf.keras.layers.Dropout(0.1)\n",
    "      self.dense = tf.keras.layers.Dense(num_classes)\n",
    "\n",
    "    def call(self, preprocessed_text):\n",
    "      encoder_outputs = self.encoder(preprocessed_text)\n",
    "      pooled_output = encoder_outputs[\"pooled_output\"]\n",
    "      x = self.dropout(pooled_output)\n",
    "      x = self.dense(x)\n",
    "      return x\n",
    "\n",
    "  model = Classifier(num_classes)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6beafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfds_name = 'scrolls/contract_nli' #change according to the task\n",
    "\n",
    "tfds_info = tfds.builder(tfds_name).info\n",
    "\n",
    "sentence_features = list(tfds_info.features.keys())\n",
    "sentence_features.remove('idx')\n",
    "sentence_features.remove('label')\n",
    "\n",
    "available_splits = list(tfds_info.splits.keys())\n",
    "train_split = 'train'\n",
    "validation_split = 'validation'\n",
    "test_split = 'test'\n",
    "if tfds_name == 'scrolls/contract_nli':\n",
    "  validation_split = 'validation_matched'\n",
    "  test_split = 'test_matched'\n",
    "\n",
    "num_classes = tfds_info.features['label'].num_classes\n",
    "num_examples = tfds_info.splits.total_num_examples\n",
    "\n",
    "print(f'Using {tfds_name} from TFDS')\n",
    "print(f'This dataset has {num_examples} examples')\n",
    "print(f'Number of classes: {num_classes}')\n",
    "print(f'Features {sentence_features}')\n",
    "print(f'Splits {available_splits}')\n",
    "\n",
    "with tf.device('/job:localhost'):\n",
    "  # batch_size=-1 is a way to load the dataset into memory\n",
    "  in_memory_ds = tfds.load(tfds_name, batch_size=-1, shuffle_files=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f466542d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact_match_fn(y_true, y_logits, threshold):\n",
    "    #pred = tf.equal(tf.round(y_logits), tf.round(y_true))\n",
    "    predictions = tf.to_float(tf.greater_equal(y_logits, threshold))\n",
    "    pred_match = tf.equal(predictions, tf.round(y_true))\n",
    "    exact_match = tf.reduce_min(tf.to_float(pred_match), axis=1)\n",
    "    return exact_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9368309",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_configuration(glue_task):\n",
    "\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "    metrics = exact_match()\n",
    "    \n",
    "    return loss\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2c19d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "batch_size = 32\n",
    "init_lr = 2e-5\n",
    "\n",
    "print(f'Fine tuning {tfhub_handle_encoder} model')\n",
    "bert_preprocess_model = make_bert_preprocess_model(sentence_features)\n",
    "\n",
    "with strategy.scope():\n",
    "\n",
    "  # metric have to be created inside the strategy scope\n",
    "  metrics, loss = get_configuration(tfds_name)\n",
    "\n",
    "  train_dataset, train_data_size = load_dataset_from_tfds(\n",
    "      in_memory_ds, tfds_info, train_split, batch_size, bert_preprocess_model)\n",
    "  steps_per_epoch = train_data_size // batch_size\n",
    "  num_train_steps = steps_per_epoch * epochs\n",
    "  num_warmup_steps = num_train_steps // 10\n",
    "\n",
    "  validation_dataset, validation_data_size = load_dataset_from_tfds(\n",
    "      in_memory_ds, tfds_info, validation_split, batch_size,\n",
    "      bert_preprocess_model)\n",
    "  validation_steps = validation_data_size // batch_size\n",
    "\n",
    "  classifier_model = build_classifier_model(num_classes)\n",
    "\n",
    "  optimizer = optimization.create_optimizer(\n",
    "      init_lr=init_lr,\n",
    "      num_train_steps=num_train_steps,\n",
    "      num_warmup_steps=num_warmup_steps,\n",
    "      optimizer_type='adamw')\n",
    "\n",
    "  classifier_model.compile(optimizer=optimizer, loss=loss, metrics=exact_match_fn)\n",
    "\n",
    "  classifier_model.fit(\n",
    "      x=train_dataset,\n",
    "      validation_data=validation_dataset,\n",
    "      steps_per_epoch=steps_per_epoch,\n",
    "      epochs=epochs,\n",
    "      validation_steps=validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fe53d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_save_path = './my_models'\n",
    "bert_type = tfhub_handle_encoder.split('/')[-2]\n",
    "saved_model_name = f'{tfds_name.replace(\"/\", \"_\")}_{bert_type}'\n",
    "\n",
    "saved_model_path = os.path.join(main_save_path, saved_model_name)\n",
    "\n",
    "preprocess_inputs = bert_preprocess_model.inputs\n",
    "bert_encoder_inputs = bert_preprocess_model(preprocess_inputs)\n",
    "bert_outputs = classifier_model(bert_encoder_inputs)\n",
    "model_for_export = tf.keras.Model(preprocess_inputs, bert_outputs)\n",
    "\n",
    "print('Saving', saved_model_path)\n",
    "\n",
    "# Save everything on the Colab host (even the variables from TPU memory)\n",
    "save_options = tf.saved_model.SaveOptions(experimental_io_device='/job:localhost')\n",
    "model_for_export.save(saved_model_path, include_optimizer=False,\n",
    "                      options=save_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa48f9a",
   "metadata": {},
   "source": [
    "### Paramixer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9db98dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mixer.paramixer import Paramixer\n",
    "para_mixer_block = Paramixer(embedding_size= 1024,\n",
    "    max_seq_len=1024,\n",
    "    n_layers=12,          \n",
    "    dropout1_p=0.3,\n",
    "    dropout2_p=0.4,\n",
    "    pooling_type='maxpool',\n",
    "    hidden_size=768)\n",
    "\n",
    "def build_classifier_model(num_classes):\n",
    "\n",
    "  class Classifier(tf.keras.Model):\n",
    "    def __init__(self, num_classes):\n",
    "      super(Classifier, self).__init__(name=\"prediction\")\n",
    "      self.encoder = para_mixer_block\n",
    "      self.dropout = tf.keras.layers.Dropout(0.1)\n",
    "      self.dense = tf.keras.layers.Dense(num_classes)\n",
    "\n",
    "    def call(self, preprocessed_text):\n",
    "      encoder_outputs = self.encoder(preprocessed_text)\n",
    "      pooled_output = encoder_outputs[\"pooled_output\"]\n",
    "      x = self.dropout(pooled_output)\n",
    "      x = self.dense(x)\n",
    "      return x\n",
    "\n",
    "  model = Classifier(num_classes)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee6eec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfds_name = 'scrolls/contract_nli' #change according to the task\n",
    "\n",
    "tfds_info = tfds.builder(tfds_name).info\n",
    "\n",
    "sentence_features = list(tfds_info.features.keys())\n",
    "sentence_features.remove('idx')\n",
    "sentence_features.remove('label')\n",
    "\n",
    "available_splits = list(tfds_info.splits.keys())\n",
    "train_split = 'train'\n",
    "validation_split = 'validation'\n",
    "test_split = 'test'\n",
    "if tfds_name == 'scrolls/contract_nli':\n",
    "  validation_split = 'validation_matched'\n",
    "  test_split = 'test_matched'\n",
    "\n",
    "num_classes = tfds_info.features['label'].num_classes\n",
    "num_examples = tfds_info.splits.total_num_examples\n",
    "\n",
    "print(f'Using {tfds_name} from TFDS')\n",
    "print(f'This dataset has {num_examples} examples')\n",
    "print(f'Number of classes: {num_classes}')\n",
    "print(f'Features {sentence_features}')\n",
    "print(f'Splits {available_splits}')\n",
    "\n",
    "with tf.device('/job:localhost'):\n",
    "  # batch_size=-1 is a way to load the dataset into memory\n",
    "  in_memory_ds = tfds.load(tfds_name, batch_size=-1, shuffle_files=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fd60f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact_match_fn(y_true, y_logits, threshold):\n",
    "    #pred = tf.equal(tf.round(y_logits), tf.round(y_true))\n",
    "    predictions = tf.to_float(tf.greater_equal(y_logits, threshold))\n",
    "    pred_match = tf.equal(predictions, tf.round(y_true))\n",
    "    exact_match = tf.reduce_min(tf.to_float(pred_match), axis=1)\n",
    "    return exact_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926635f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_configuration(glue_task):\n",
    "\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "    metrics = exact_match()\n",
    "    \n",
    "    return loss\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1f194d",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "batch_size = 32\n",
    "init_lr = 2e-5\n",
    "\n",
    "print(f'Fine tuning {tfhub_handle_encoder} model')\n",
    "bert_preprocess_model = make_bert_preprocess_model(sentence_features)\n",
    "\n",
    "with strategy.scope():\n",
    "\n",
    "  # metric have to be created inside the strategy scope\n",
    "  metrics, loss = get_configuration(tfds_name)\n",
    "\n",
    "  train_dataset, train_data_size = load_dataset_from_tfds(\n",
    "      in_memory_ds, tfds_info, train_split, batch_size, bert_preprocess_model)\n",
    "  steps_per_epoch = train_data_size // batch_size\n",
    "  num_train_steps = steps_per_epoch * epochs\n",
    "  num_warmup_steps = num_train_steps // 10\n",
    "\n",
    "  validation_dataset, validation_data_size = load_dataset_from_tfds(\n",
    "      in_memory_ds, tfds_info, validation_split, batch_size,\n",
    "      bert_preprocess_model)\n",
    "  validation_steps = validation_data_size // batch_size\n",
    "\n",
    "  classifier_model = build_classifier_model(num_classes)\n",
    "\n",
    "  optimizer = optimization.create_optimizer(\n",
    "      init_lr=init_lr,\n",
    "      num_train_steps=num_train_steps,\n",
    "      num_warmup_steps=num_warmup_steps,\n",
    "      optimizer_type='adamw')\n",
    "\n",
    "  classifier_model.compile(optimizer=optimizer, loss=loss, metrics=exact_match_fn)\n",
    "\n",
    "  classifier_model.fit(\n",
    "      x=train_dataset,\n",
    "      validation_data=validation_dataset,\n",
    "      steps_per_epoch=steps_per_epoch,\n",
    "      epochs=epochs,\n",
    "      validation_steps=validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c3f320",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_save_path = './my_models'\n",
    "bert_type = tfhub_handle_encoder.split('/')[-2]\n",
    "saved_model_name = f'{tfds_name.replace(\"/\", \"_\")}_{bert_type}'\n",
    "\n",
    "saved_model_path = os.path.join(main_save_path, saved_model_name)\n",
    "\n",
    "preprocess_inputs = bert_preprocess_model.inputs\n",
    "bert_encoder_inputs = bert_preprocess_model(preprocess_inputs)\n",
    "bert_outputs = classifier_model(bert_encoder_inputs)\n",
    "model_for_export = tf.keras.Model(preprocess_inputs, bert_outputs)\n",
    "\n",
    "print('Saving', saved_model_path)\n",
    "\n",
    "# Save everything on the Colab host (even the variables from TPU memory)\n",
    "save_options = tf.saved_model.SaveOptions(experimental_io_device='/job:localhost')\n",
    "model_for_export.save(saved_model_path, include_optimizer=False,\n",
    "                      options=save_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6634256",
   "metadata": {},
   "source": [
    "### MorphMLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6497078f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mixer.MorphMLP import MorphMLP\n",
    "morph_mlp_block = MorphMLP(dim = 1024)\n",
    "\n",
    "\n",
    "def build_classifier_model(num_classes):\n",
    "\n",
    "  class Classifier(tf.keras.Model):\n",
    "    def __init__(self, num_classes):\n",
    "      super(Classifier, self).__init__(name=\"prediction\")\n",
    "      self.encoder = morph_mlp_block\n",
    "      self.dropout = tf.keras.layers.Dropout(0.1)\n",
    "      self.dense = tf.keras.layers.Dense(num_classes)\n",
    "\n",
    "    def call(self, preprocessed_text):\n",
    "      encoder_outputs = self.encoder(preprocessed_text)\n",
    "      pooled_output = encoder_outputs[\"pooled_output\"]\n",
    "      x = self.dropout(pooled_output)\n",
    "      x = self.dense(x)\n",
    "      return x\n",
    "\n",
    "  model = Classifier(num_classes)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b1e1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfds_name = 'scrolls/contract_nli' #change according to the task\n",
    "\n",
    "tfds_info = tfds.builder(tfds_name).info\n",
    "\n",
    "sentence_features = list(tfds_info.features.keys())\n",
    "sentence_features.remove('idx')\n",
    "sentence_features.remove('label')\n",
    "\n",
    "available_splits = list(tfds_info.splits.keys())\n",
    "train_split = 'train'\n",
    "validation_split = 'validation'\n",
    "test_split = 'test'\n",
    "if tfds_name == 'scrolls/contract_nli':\n",
    "  validation_split = 'validation_matched'\n",
    "  test_split = 'test_matched'\n",
    "\n",
    "num_classes = tfds_info.features['label'].num_classes\n",
    "num_examples = tfds_info.splits.total_num_examples\n",
    "\n",
    "print(f'Using {tfds_name} from TFDS')\n",
    "print(f'This dataset has {num_examples} examples')\n",
    "print(f'Number of classes: {num_classes}')\n",
    "print(f'Features {sentence_features}')\n",
    "print(f'Splits {available_splits}')\n",
    "\n",
    "with tf.device('/job:localhost'):\n",
    "  # batch_size=-1 is a way to load the dataset into memory\n",
    "  in_memory_ds = tfds.load(tfds_name, batch_size=-1, shuffle_files=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b6b789",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact_match_fn(y_true, y_logits, threshold):\n",
    "    #pred = tf.equal(tf.round(y_logits), tf.round(y_true))\n",
    "    predictions = tf.to_float(tf.greater_equal(y_logits, threshold))\n",
    "    pred_match = tf.equal(predictions, tf.round(y_true))\n",
    "    exact_match = tf.reduce_min(tf.to_float(pred_match), axis=1)\n",
    "    return exact_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b6ac9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_configuration(glue_task):\n",
    "\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "    metrics = exact_match()\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5313c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "batch_size = 32\n",
    "init_lr = 2e-5\n",
    "\n",
    "print(f'Fine tuning {tfhub_handle_encoder} model')\n",
    "bert_preprocess_model = make_bert_preprocess_model(sentence_features)\n",
    "\n",
    "with strategy.scope():\n",
    "\n",
    "  # metric have to be created inside the strategy scope\n",
    "  metrics, loss = get_configuration(tfds_name)\n",
    "\n",
    "  train_dataset, train_data_size = load_dataset_from_tfds(\n",
    "      in_memory_ds, tfds_info, train_split, batch_size, bert_preprocess_model)\n",
    "  steps_per_epoch = train_data_size // batch_size\n",
    "  num_train_steps = steps_per_epoch * epochs\n",
    "  num_warmup_steps = num_train_steps // 10\n",
    "\n",
    "  validation_dataset, validation_data_size = load_dataset_from_tfds(\n",
    "      in_memory_ds, tfds_info, validation_split, batch_size,\n",
    "      bert_preprocess_model)\n",
    "  validation_steps = validation_data_size // batch_size\n",
    "\n",
    "  classifier_model = build_classifier_model(num_classes)\n",
    "\n",
    "  optimizer = optimization.create_optimizer(\n",
    "      init_lr=init_lr,\n",
    "      num_train_steps=num_train_steps,\n",
    "      num_warmup_steps=num_warmup_steps,\n",
    "      optimizer_type='adamw')\n",
    "\n",
    "  classifier_model.compile(optimizer=optimizer, loss=loss, metrics=exact_match_fn)\n",
    "\n",
    "  classifier_model.fit(\n",
    "      x=train_dataset,\n",
    "      validation_data=validation_dataset,\n",
    "      steps_per_epoch=steps_per_epoch,\n",
    "      epochs=epochs,\n",
    "      validation_steps=validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9151cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_save_path = './my_models'\n",
    "bert_type = tfhub_handle_encoder.split('/')[-2]\n",
    "saved_model_name = f'{tfds_name.replace(\"/\", \"_\")}_{bert_type}'\n",
    "\n",
    "saved_model_path = os.path.join(main_save_path, saved_model_name)\n",
    "\n",
    "preprocess_inputs = bert_preprocess_model.inputs\n",
    "bert_encoder_inputs = bert_preprocess_model(preprocess_inputs)\n",
    "bert_outputs = classifier_model(bert_encoder_inputs)\n",
    "model_for_export = tf.keras.Model(preprocess_inputs, bert_outputs)\n",
    "\n",
    "print('Saving', saved_model_path)\n",
    "\n",
    "# Save everything on the Colab host (even the variables from TPU memory)\n",
    "save_options = tf.saved_model.SaveOptions(experimental_io_device='/job:localhost')\n",
    "model_for_export.save(saved_model_path, include_optimizer=False,\n",
    "                      options=save_options)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
